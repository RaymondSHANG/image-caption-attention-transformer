model:
  # model_1:CNN+LSTM+Att
  # model_3:CNN+LSTM+withouAtt
  # model_4:CNN+transformer_encoder
  # model_5:CNN+transformer_decoderCrossAtt
  # model_6:CNN+transformer_decoder
  # model_7:CNN+transformer_complete
  model_name: model_5
  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead
  cudnn_benchmark: True
model_para:
  emb_dim: 512 # dimension of word embeddings
  attention_dim: 512 # dimension of attention linear layers
  decoder_dim: 512 # dimension of decoder RNN
  dropout: 0.5

data:
  data_name: coco_5_cap_per_img_5_min_word_freq
  data_folder: /media/yshang/T7/imgCap # regular or imbalance
  checkpoint_folder: /media/yshang/T7/imgCap/checkpoints
  save_best: True

loss:
  loss_type: CE # CE or Focal

Train:
  start_epoch: 0
  # number of epochs to train for (if early stopping is not triggered)
  epochs: 25
  # keeps track of number of epochs since there's been an improvement in validation BLEU
  epochs_since_improvement: 0
  batch_size: 128
  workers: 1 # for data-loading; right now, only 1 works with h5py
  encoder_lr: !!float 1e-4 # learning rate for encoder if fine-tuning
  decoder_lr: !!float 4e-4 # learning rate for decoder
  grad_clip: 5. # clip gradients at an absolute value of
  alpha_c: 1. # regularization parameter for 'doubly stochastic attention', as in the paper
  best_bleu4: 0. # BLEU-4 score right now
  print_freq: 500 # print training/validation stats every __ batches
  fine_tune_encoder: True # fine-tune encoder?
  # path to checkpoint, None if none
  # '/home/yshang/Documents/imgcap/checkpoints/legacy_BEST_checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar'
  checkpoint: BEST_checkpoint_model_5_coco_5_cap_per_img_5_min_word_freq.pth.tar

Eval:
  checkpoint_eval: Best #Best or Last
  beam_size_eval: 5

Caption:
  checkpoint_caption: Best #Best or Last
  beam_size_caption: 5
  img: myimg/img1.jpeg
  smooth: False # smooth alpha overlay?
